{"version":3,"sources":["app/index.js","app/initialize.js"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AA9HA;AAAA;ACAA;AAAA;AACA;AADA;AAAA","file":"public/app.js","sourcesContent":["\"use strict\";\n\n$(document).ready(function () {\n  // SDK Needs to create video and canvas nodes in the DOM in order to function\n  // Here we are adding those nodes a predefined div.\n  var divRoot = $(\"#affdex_elements\")[0];\n  var width = 340;\n  var height = 240;\n  var faceMode = affdex.FaceDetectorMode.LARGE_FACES;\n  //Construct a CameraDetector and specify the image width / height and face detector mode.\n  var detector = new affdex.CameraDetector(divRoot, width, height, faceMode);\n\n  //Enable detection of all Expressions, Emotions and Emojis classifiers.\n  detector.detectAllEmotions();\n  detector.detectAllExpressions();\n  detector.detectAllEmojis();\n  detector.detectAllAppearance();\n\n  //Add a callback to notify when the detector is initialized and ready for runing.\n  detector.addEventListener(\"onInitializeSuccess\", function () {\n    console.log(\"The detector reports initialized\");\n    //Display canvas instead of video feed because we want to draw the feature points on it\n    $(\"#face_video_canvas\").css(\"display\", \"block\");\n    $(\"#face_video\").css(\"display\", \"none\");\n  });\n\n  function log(node_name, msg) {\n    $(node_name).append(\"<span>\" + msg + \"</span><br />\");\n  }\n\n  //function executes when Start button is pushed.\n  $(\"#start\").click(function () {\n    if (detector && !detector.isRunning) {\n      $(\"#logs\").html(\"\");\n      detector.start();\n    }\n    console.log(\"Clicked the start button\");\n  });\n\n  //function executes when the Stop button is pushed.\n  $(\"#stop\").click(function () {\n    console.log(\"Clicked the stop button\");\n    if (detector && detector.isRunning) {\n      detector.removeEventListener();\n      detector.stop();\n    }\n  });\n\n  //function executes when the Reset button is pushed.\n  $(\"#reset\").click(function () {\n    console.log(\"Clicked the reset button\");\n    if (detector && detector.isRunning) {\n      detector.reset();\n\n      $('#results').html(\"\");\n    }\n  });\n\n  //Add a callback to notify when camera access is allowed\n  detector.addEventListener(\"onWebcamConnectSuccess\", function () {\n    console.log(\"Webcam access allowed\");\n  });\n\n  //Add a callback to notify when camera access is denied\n  detector.addEventListener(\"onWebcamConnectFailure\", function () {\n    console.log(\"Webcam access denied\");\n  });\n\n  //Add a callback to notify when detector is stopped\n  detector.addEventListener(\"onStopSuccess\", function () {\n    console.log(\"The detector reports stopped\");\n    $(\"#results\").html(\"\");\n  });\n\n  //Add a callback to receive the results from processing an image.\n  //The faces object contains the list of the faces detected in an image.\n  //Faces object contains probabilities for all the different expressions, emotions and appearance metrics\n  detector.addEventListener(\"onImageResultsSuccess\", function (faces, image, timestamp) {\n    $('#results').html(\"\");\n    log('#results', \"Timestamp: \" + timestamp.toFixed(2));\n    log('#results', \"Number of faces found: \" + faces.length);\n    if (faces.length > 0) {\n      log('#results', \"Appearance: \" + JSON.stringify(faces[0].appearance));\n      log('#results', \"Emotions: \" + JSON.stringify(faces[0].emotions, function (key, val) {\n        return val.toFixed ? Number(val.toFixed(0)) : val;\n      }));\n      log('#results', \"Expressions: \" + JSON.stringify(faces[0].expressions, function (key, val) {\n        return val.toFixed ? Number(val.toFixed(0)) : val;\n      }));\n      log('#results', \"Emoji: \" + faces[0].emojis.dominantEmoji);\n      drawFeaturePoints(image, faces[0].featurePoints, faces[0].emotions);\n    }\n  });\n\n  //Draw the detected facial feature points on the image\n  function drawFeaturePoints(img, featurePoints, emotions) {\n    var contxt = $('#peter')[0].getContext('2d');\n\n    var hRatio = contxt.canvas.width / img.width;\n    var vRatio = contxt.canvas.height / img.height;\n    var ratio = Math.min(hRatio, vRatio);\n\n    contxt.strokeStyle = \"#FFFFFF\";\n    for (var id in featurePoints) {\n      if (id == 12 || id == 30 || id == 32) {\n        contxt.beginPath();\n        contxt.arc(featurePoints[id].x, featurePoints[id].y, 1, 0, 2 * Math.PI);\n        contxt.closePath();\n        var red = Math.round(emotions.joy / 100 * 255);\n        var green = Math.round(emotions.contempt / 100 * 255);\n        var blue = Math.round(emotions.engagement / 100 * 255);\n        contxt.fillStyle = \"rgba(\" + red + \",\" + green + \",\" + blue + \", 1)\";\n        contxt.fill();\n      }\n    }\n  }\n\n  function fadeOut() {\n    var ctx = $('#peter')[0].getContext('2d');\n    ctx.fillStyle = \"rgba(255,255,255,0.1)\";\n    ctx.fillRect(0, 0, width, height);\n    setTimeout(fadeOut, 1000);\n  }\n\n  fadeOut();\n});\n","\"use strict\";\n"]}